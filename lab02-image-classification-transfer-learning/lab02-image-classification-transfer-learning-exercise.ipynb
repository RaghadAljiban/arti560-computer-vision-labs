{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "430e7f7b",
   "metadata": {},
   "source": [
    "##### ARTI 560 - Computer Vision  \n",
    "## Image Classification using Transfer Learning - Exercise \n",
    "\n",
    "### Objective\n",
    "\n",
    "In this exercise, you will:\n",
    "\n",
    "1. Select another pretrained model (e.g., VGG16, MobileNetV2, or EfficientNet) and fine-tune it for CIFAR-10 classification.  \n",
    "You'll find the pretrained models in [Tensorflow Keras Applications Module](https://www.tensorflow.org/api_docs/python/tf/keras/applications).\n",
    "\n",
    "2. Before training, inspect the architecture using model.summary() and observe:\n",
    "- Network depth\n",
    "- Number of parameters\n",
    "- Trainable vs Frozen layers\n",
    "\n",
    "3. Then compare its performance with ResNet and the custom CNN.\n",
    "\n",
    "### Questions:\n",
    "\n",
    "- Which model achieved the highest accuracy?\n",
    "- Which model trained faster?\n",
    "- How might the architecture explain the differences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a26d77e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e682940",
   "metadata": {},
   "source": [
    "### Answers: ###\n",
    "1- The ResNet50V2 (fine-tuned) model achieved the highest test accuracy at 91.62%, slightly outperforming the custom CNN (91.45%).\n",
    "2- The custom CNN trained the fastest, followed by MobileNetV2, while ResNet50V2 required the most training time due to its depth and complexity.\n",
    "3- ResNet50V2 is a deep architecture with residual connections that allow it to learn complex representations, leading to higher accuracy but slower training. MobileNetV2 uses depthwise separable convolutions to reduce computation, making it faster but slightly less accurate. The custom CNN is shallow and computationally simple, which explains its fast training time but slightly lower performance compared to fine-tuned ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d197d1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"cifar10_resnet50v2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"cifar10_resnet50v2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ augmentation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ resizing (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Resizing</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,810</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ augmentation (\u001b[38;5;33mSequential\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ resizing (\u001b[38;5;33mResizing\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │        \u001b[38;5;34m12,810\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,270,794</span> (8.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,270,794\u001b[0m (8.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,810</span> (50.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,810\u001b[0m (50.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 105ms/step - accuracy: 0.6055 - loss: 1.1498 - val_accuracy: 0.8062 - val_loss: 0.5751 - learning_rate: 0.0010\n",
      "Epoch 2/3\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 108ms/step - accuracy: 0.7407 - loss: 0.7423 - val_accuracy: 0.8306 - val_loss: 0.5099 - learning_rate: 0.0010\n",
      "Epoch 3/3\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 108ms/step - accuracy: 0.7591 - loss: 0.6938 - val_accuracy: 0.8226 - val_loss: 0.5065 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load CIFAR-10\n",
    "# -----------------------------\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "class_names = [\n",
    "    \"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\n",
    "    \"dog\",\"frog\",\"horse\",\"ship\",\"truck\"\n",
    "]\n",
    "\n",
    "# Keep labels as integers (SparseCategoricalCrossentropy)\n",
    "y_train = y_train.squeeze().astype(\"int64\")\n",
    "y_test  = y_test.squeeze().astype(\"int64\")\n",
    "\n",
    "# Convert images to float32\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test  = x_test.astype(\"float32\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Data augmentation\n",
    "# -----------------------------\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.05),\n",
    "    layers.RandomZoom(0.1),\n",
    "], name=\"augmentation\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Build MobileNetV2 backbone (pretrained)\n",
    "# -----------------------------\n",
    "resnet_base = MobileNetV2(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "resnet_base.trainable = False  # freeze first (feature extractor)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Full model (preprocess inside model)\n",
    "# -----------------------------\n",
    "resnet_model = keras.Sequential([\n",
    "    layers.Input(shape=(32, 32, 3)),\n",
    "    data_augmentation,\n",
    "    layers.Resizing(224, 224, interpolation=\"bilinear\"),\n",
    "    layers.Lambda(preprocess_input),          # IMPORTANT\n",
    "    resnet_base,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(10)                          \n",
    "], name=\"cifar10_resnet50v2\")\n",
    "\n",
    "resnet_model.summary()\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Compile + Train (frozen backbone)\n",
    "# -----------------------------\n",
    "resnet_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=3, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=1),\n",
    "]\n",
    "\n",
    "history = resnet_model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=3,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08803d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2 (frozen) test accuracy: 0.8170999884605408\n",
      "MobileNetV2 (frozen) test loss    : 0.5334654450416565\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 6) Test / Evaluate\n",
    "# -----------------------------\n",
    "test_loss, test_acc_r = resnet_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"MobileNetV2 (frozen) test accuracy:\", test_acc_r)\n",
    "print(\"MobileNetV2 (frozen) test loss    :\", test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1539b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total layers in MobileNetV2 backbone: 154\n",
      "Layers with learnable parameters (depth): 104\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of layers inside the MobileNetV2 backbone\n",
    "print(\"Total layers in MobileNetV2 backbone:\", len(resnet_base.layers))\n",
    "\n",
    "# Filter only layers that actually have learnable parameters (weights/biases)\n",
    "trainable_layers = [layer for layer in resnet_base.layers if layer.count_params() > 0]\n",
    "    \n",
    "# Print the number of layers that contain learnable parameters \"Depth of the Model\"\n",
    "# It will be 102 (not 103) because MobileNetV2's classification head is NOT included as we are using only the backbone (feature extractor)\n",
    "print(\"Layers with learnable parameters (depth):\", len(trainable_layers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20e2ea40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Conv1 864\n",
      "1 bn_Conv1 128\n",
      "2 expanded_conv_depthwise 288\n",
      "3 expanded_conv_depthwise_BN 128\n",
      "4 expanded_conv_project 512\n",
      "5 expanded_conv_project_BN 64\n",
      "6 block_1_expand 1536\n",
      "7 block_1_expand_BN 384\n",
      "8 block_1_depthwise 864\n",
      "9 block_1_depthwise_BN 384\n",
      "10 block_1_project 2304\n",
      "11 block_1_project_BN 96\n",
      "12 block_2_expand 3456\n",
      "13 block_2_expand_BN 576\n",
      "14 block_2_depthwise 1296\n",
      "15 block_2_depthwise_BN 576\n",
      "16 block_2_project 3456\n",
      "17 block_2_project_BN 96\n",
      "18 block_3_expand 3456\n",
      "19 block_3_expand_BN 576\n",
      "20 block_3_depthwise 1296\n",
      "21 block_3_depthwise_BN 576\n",
      "22 block_3_project 4608\n",
      "23 block_3_project_BN 128\n",
      "24 block_4_expand 6144\n",
      "25 block_4_expand_BN 768\n",
      "26 block_4_depthwise 1728\n",
      "27 block_4_depthwise_BN 768\n",
      "28 block_4_project 6144\n",
      "29 block_4_project_BN 128\n",
      "30 block_5_expand 6144\n",
      "31 block_5_expand_BN 768\n",
      "32 block_5_depthwise 1728\n",
      "33 block_5_depthwise_BN 768\n",
      "34 block_5_project 6144\n",
      "35 block_5_project_BN 128\n",
      "36 block_6_expand 6144\n",
      "37 block_6_expand_BN 768\n",
      "38 block_6_depthwise 1728\n",
      "39 block_6_depthwise_BN 768\n",
      "40 block_6_project 12288\n",
      "41 block_6_project_BN 256\n",
      "42 block_7_expand 24576\n",
      "43 block_7_expand_BN 1536\n",
      "44 block_7_depthwise 3456\n",
      "45 block_7_depthwise_BN 1536\n",
      "46 block_7_project 24576\n",
      "47 block_7_project_BN 256\n",
      "48 block_8_expand 24576\n",
      "49 block_8_expand_BN 1536\n",
      "50 block_8_depthwise 3456\n",
      "51 block_8_depthwise_BN 1536\n",
      "52 block_8_project 24576\n",
      "53 block_8_project_BN 256\n",
      "54 block_9_expand 24576\n",
      "55 block_9_expand_BN 1536\n",
      "56 block_9_depthwise 3456\n",
      "57 block_9_depthwise_BN 1536\n",
      "58 block_9_project 24576\n",
      "59 block_9_project_BN 256\n",
      "60 block_10_expand 24576\n",
      "61 block_10_expand_BN 1536\n",
      "62 block_10_depthwise 3456\n",
      "63 block_10_depthwise_BN 1536\n",
      "64 block_10_project 36864\n",
      "65 block_10_project_BN 384\n",
      "66 block_11_expand 55296\n",
      "67 block_11_expand_BN 2304\n",
      "68 block_11_depthwise 5184\n",
      "69 block_11_depthwise_BN 2304\n",
      "70 block_11_project 55296\n",
      "71 block_11_project_BN 384\n",
      "72 block_12_expand 55296\n",
      "73 block_12_expand_BN 2304\n",
      "74 block_12_depthwise 5184\n",
      "75 block_12_depthwise_BN 2304\n",
      "76 block_12_project 55296\n",
      "77 block_12_project_BN 384\n",
      "78 block_13_expand 55296\n",
      "79 block_13_expand_BN 2304\n",
      "80 block_13_depthwise 5184\n",
      "81 block_13_depthwise_BN 2304\n",
      "82 block_13_project 92160\n",
      "83 block_13_project_BN 640\n",
      "84 block_14_expand 153600\n",
      "85 block_14_expand_BN 3840\n",
      "86 block_14_depthwise 8640\n",
      "87 block_14_depthwise_BN 3840\n",
      "88 block_14_project 153600\n",
      "89 block_14_project_BN 640\n",
      "90 block_15_expand 153600\n",
      "91 block_15_expand_BN 3840\n",
      "92 block_15_depthwise 8640\n",
      "93 block_15_depthwise_BN 3840\n",
      "94 block_15_project 153600\n",
      "95 block_15_project_BN 640\n",
      "96 block_16_expand 153600\n",
      "97 block_16_expand_BN 3840\n",
      "98 block_16_depthwise 8640\n",
      "99 block_16_depthwise_BN 3840\n",
      "100 block_16_project 307200\n",
      "101 block_16_project_BN 1280\n",
      "102 Conv_1 409600\n",
      "103 Conv_1_bn 5120\n"
     ]
    }
   ],
   "source": [
    "# Listing all layers that have learnable parameters (trainable_layers)\n",
    "# Each layer will be printed with:\n",
    "# (index in the filtered list, layer name, number of parameters)\n",
    "for i, layer in enumerate(trainable_layers):\n",
    "    print(i, layer.name, layer.count_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bd2a3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable layers in backbone: 30 / 154\n",
      "Epoch 1/3\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 142ms/step - accuracy: 0.6725 - loss: 0.9588 - val_accuracy: 0.8260 - val_loss: 0.5176\n",
      "Epoch 2/3\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 139ms/step - accuracy: 0.7699 - loss: 0.6599 - val_accuracy: 0.8354 - val_loss: 0.4656\n",
      "Epoch 3/3\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 140ms/step - accuracy: 0.7982 - loss: 0.5786 - val_accuracy: 0.8496 - val_loss: 0.4163\n",
      "MobileNetV2 (fine-tuned) test accuracy: 0.8514999747276306\n",
      "MobileNetV2 (fine-tuned) test loss    : 0.4299635887145996\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "#Fine-tune last layers\n",
    "# -----------------------------\n",
    "resnet_base.trainable = True\n",
    "for layer in resnet_base.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(\"Trainable layers in backbone:\", sum(l.trainable for l in resnet_base.layers), \"/\", len(resnet_base.layers))\n",
    "\n",
    "resnet_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history_ft = resnet_model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=3,\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "test_loss_ft, test_acc_ft = resnet_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"MobileNetV2 (fine-tuned) test accuracy:\", test_acc_ft)\n",
    "print(\"MobileNetV2 (fine-tuned) test loss    :\", test_loss_ft)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
